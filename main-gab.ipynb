{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project: French Grand Debat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Introduction :\n",
    "\n",
    "First, we would like to choose a problematic according to the data we collected. The aim of this project could be to answer the following questions:\n",
    "\n",
    "* What are the 5 most important ideas on each theme?\n",
    "* Can we build different profiles of people with their ideas on the 4 themes?\n",
    "* Can we predict themes about which people could be interested in if those people answered questions only about 3 on 4 themes? \n",
    " \n",
    "#### Doability :\n",
    "\n",
    "We are going to import the data, then check if the ids of people who submitted ideas on different themes are the same. If not, we won't be able to answer the second question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librairies imported\n",
    "import src.utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from src.kmeans_embeddings import FeaturesExtractor\n",
    "from sklearn.cluster import KMeans\n",
    "from src.utils import (read_data, get_open_reponses, get_ids_open_reponses)\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_open_reponses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6b4a74297227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# open responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_resp_fis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_open_reponses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fiscalite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf_resp_dem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_open_reponses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_democratie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf_resp_eco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_open_reponses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ecologie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_open_reponses' is not defined"
     ]
    }
   ],
   "source": [
    "df_fiscalite = ut.read_data('data/LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.json')\n",
    "df_democratie = ut.read_data('data/DEMOCRATIE_ET_CITOYENNETE.json')\n",
    "df_ecologie = ut.read_data('data/LA_TRANSITION_ECOLOGIQUE.json')\n",
    "df_organisation = ut.read_data('data/ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.json')\n",
    "\n",
    "dfs = np.array([[\"fiscalite\", df_fiscalite], [\"democratie\", df_democratie], [\"ecologie\", df_ecologie], [\"organisation\", df_organisation]])\n",
    "\n",
    "# open responses\n",
    "df_resp_fis = get_open_reponses(df_fiscalite)\n",
    "df_resp_dem = get_open_reponses(df_democratie)\n",
    "df_resp_eco = get_open_reponses(df_ecologie)\n",
    "df_resp_org = get_open_reponses(df_organisation)\n",
    "\n",
    "dfs_responses = np.array([[\"responses fiscalite\", df_resp_fis], [\"responses democratie\", df_resp_dem], [\"responses ecologie\", df_resp_eco], [\"responses organisation\", df_resp_org]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author ID : VXNlcjoxMjViYWQ4Yi0xZmM0LTExZTktOTRkMi1mYTE2M2VlYjExZTE=\n",
      "* In fiscalite survey, author has zip code : 93320\n",
      "* In democratie survey, author has not answered...\n",
      "* In ecologie survey, author has not answered...\n",
      "* In organisation survey, author has not answered...\n",
      "\n",
      "############################\n",
      "\n",
      "Author ID : VXNlcjplYWEyMzA2MC0xZGEzLTExZTktOTRkMi1mYTE2M2VlYjExZTE=\n",
      "* In fiscalite survey, author has zip code : 82290\n",
      "* In democratie survey, author has zip code : 82290\n",
      "* In ecologie survey, author has not answered...\n",
      "* In organisation survey, author has zip code : 82290\n",
      "\n",
      "############################\n",
      "\n",
      "Author ID : VXNlcjo5ZjllMTFiZS0xYTQ3LTExZTktOTRkMi1mYTE2M2VlYjExZTE=\n",
      "* In fiscalite survey, author has zip code : 59700\n",
      "* In democratie survey, author has not answered...\n",
      "* In ecologie survey, author has not answered...\n",
      "* In organisation survey, author has not answered...\n",
      "\n",
      "############################\n",
      "\n",
      "Author ID : VXNlcjplZjFhMGViMS0xZTU4LTExZTktOTRkMi1mYTE2M2VlYjExZTE=\n",
      "* In fiscalite survey, author has zip code : 13008\n",
      "* In democratie survey, author has zip code : 13008\n",
      "* In ecologie survey, author has zip code : 13008\n",
      "* In organisation survey, author has zip code : 13008\n",
      "\n",
      "############################\n",
      "\n",
      "Author ID : VXNlcjo5MTU4MjZmMS0yMTg1LTExZTktOTRkMi1mYTE2M2VlYjExZTE=\n",
      "* In fiscalite survey, author has zip code : 27670\n",
      "* In democratie survey, author has not answered...\n",
      "* In ecologie survey, author has not answered...\n",
      "* In organisation survey, author has zip code : 27670\n",
      "\n",
      "############################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "for i in np.random.randint(len(df_fiscalite), size=5):\n",
    "    auth = df_fiscalite.loc[i, 'authorId']\n",
    "    print(\"Author ID : \" + auth)\n",
    "\n",
    "    dfs = np.array([[\"fiscalite\", df_fiscalite], [\"democratie\", df_democratie], [\"ecologie\", df_ecologie], [\"organisation\", df_organisation]])\n",
    "    for df in dfs:\n",
    "        code = df[1].loc[df[1]['authorId'] == auth, 'authorZipCode']\n",
    "        if(len(code) > 0):\n",
    "            code = code.values[0]\n",
    "            print(\"* In \" + df[0] + \" survey, author has zip code : \" + str(code))\n",
    "        else:\n",
    "            print(\"* In \" + df[0] + \" survey, author has not answered...\")\n",
    "    print(\"\\n############################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the previous lines, we can see that the `authorId` is likely to be an unique id whaterver the dataframe (zip code is the same).**\n",
    "\n",
    "### How many people answered open questions of several themes ?\n",
    "\n",
    "In order to build type-profiles, we need a large number of people who answered open questions on several themes. That is what we will try to find with the following code.\n",
    "\n",
    "**Be careful, this cell takes a lot of time to run !**\n",
    "\n",
    "source file : `create_author_tabs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b0ca590499c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_auth_id_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_auth_id_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'authorId'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mauth_answers_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauth_answers_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_datetimelike_v_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# allAuthIds is the sets of all the authorIds\n",
    "allAuthIds = []\n",
    "for i in range(4):\n",
    "    allAuthIds.extend(set(dfs_responses[i,1]['authorId'].values))\n",
    "allAuthIds = set(allAuthIds)\n",
    "\n",
    "# all_auth_id_array is the sorted array of all the authorIds\n",
    "all_auth_id_array = np.sort(np.array(list(allAuthIds)))\n",
    "\n",
    "# auth_answers_count[i,j] is 1 if all_auth_id_array[i] has answered survey dfs[j]\n",
    "auth_answers_count = np.zeros((len(allAuthIds), 4), dtype=int)\n",
    "for j in range(4):\n",
    "    for i in range(len(all_auth_id_array)):\n",
    "        auth = all_auth_id_array[i]\n",
    "        line = dfs_responses[j,1].loc[dfs_responses[j,1]['authorId'] == auth]\n",
    "        if(len(line) > 0):\n",
    "            auth_answers_count[i,j] = auth_answers_count[i,j] + 1\n",
    "\n",
    "            \n",
    "# This cell aims to save the auth_answers_count array in auth_answers_count.csv\n",
    "np.savetxt(\"auth_answers_count.csv\", auth_answers_count, fmt='%1u', delimiter=\",\")\n",
    "\n",
    "# list of the authors that answered the four themes\n",
    "four_surveys_taken_auth_ids = [all_auth_id_array[i] for i in range(len(all_auth_id_array)) if number_of_survey_taken[i] == 4]\n",
    "\n",
    "np.savetxt(\"four_surveys_taken_auth_ids.csv\", four_surveys_taken_auth_ids, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can run this cell instead to load auth_answers_count from csv :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth_answers_count :\n",
      "[[0 0 1 0]\n",
      " [1 1 1 0]\n",
      " [0 0 0 1]\n",
      " ...\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]]\n",
      "#######################\n",
      "number of participant by survey :\n",
      "fiscalite : 53961\n",
      "democratie : 32424\n",
      "ecologie : 42656\n",
      "organisation : 34529\n",
      "#######################\n",
      "number of participant to x surveys :\n",
      "0 people have participed to 0 different surveys.\n",
      "54322 people have participed to 1 different surveys.\n",
      "17401 people have participed to 2 different surveys.\n",
      "9946 people have participed to 3 different surveys.\n",
      "11152 people have participed to 4 different surveys.\n"
     ]
    }
   ],
   "source": [
    "# Read auth_answers_count from auth_answers_count.csv\n",
    "auth_answers_count = np.loadtxt('auth_answers_count.csv', dtype=int ,delimiter=\",\")\n",
    "\n",
    "print(\"auth_answers_count :\")\n",
    "print(auth_answers_count)\n",
    "\n",
    "# number_of_survey_taken[i] is the number of survey answered by all_auth_id_array[i]\n",
    "number_of_survey_taken = np.sum(auth_answers_count, axis=1)\n",
    "# number_of_participants_by_survey[i] is the number of participants to survey dfs[j]\n",
    "number_of_participants_by_survey = np.sum(auth_answers_count, axis=0)\n",
    "\n",
    "print(\"#######################\")\n",
    "print(\"number of participant by survey :\")\n",
    "for i in range(4):\n",
    "    print(dfs[i,0] + \" : \" + str(number_of_participants_by_survey[i]))\n",
    "\n",
    "# number_of_participant_to_several_surveys[i] is the number of participants that have\n",
    "# answerd to i surveys out of the 4 (0<i<5)\n",
    "number_of_participant_to_several_surveys = np.bincount(number_of_survey_taken)\n",
    "\n",
    "print(\"#######################\")\n",
    "print(\"number of participant to x surveys :\")\n",
    "for i in range(5):\n",
    "    print(str(number_of_participant_to_several_surveys[i]) + \" people have participed to \"\n",
    "          + str(i) + \" different surveys.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume\n",
    "\n",
    "Total number of participants by survey (open questions only):\n",
    "\n",
    "* **fiscalite :** 53961\n",
    "* **democratie :** 32424\n",
    "* **ecologie :** 42656\n",
    "* **organisation :** 34529\n",
    "\n",
    "Number of participant to $x$ surveys :\n",
    "\n",
    "* 54322 people have participated to only one survey\n",
    "* 17401 people have participated to 2 surveys\n",
    "* 9946 people have participated to 3 survey\n",
    "* 11152 people have participated to 4 survey\n",
    "\n",
    "These preliminaries let us think that what we wanted to do for this project is doable, because the number of people who answered on several themes is pretty large. We are now looking for a method to build these type-profiles.\n",
    "\n",
    "\n",
    "# General ideas about this project\n",
    "\n",
    "After having processed the data, our project will be articulated in two parts: \n",
    "\n",
    "* **Learning :** We are going to build a model able to find different groups (or trends opinion) based on clustering. We will train our model on people who answered the 4 themes. We will make some tests to determine a number of different profiles (maximum of 10 clusters) that seems relevant. Then we will check the relevance of these type profiles.\n",
    "* **Check :** We will test our model applying it to the test dataset (people who have participated to 3 surveys) to check if it is able to correctly predict the themes.\n",
    "\n",
    "Based on the result of the last point we will be able to say whether or not our goal is reached, and if the dataset given is enough to achieve this.\n",
    "\n",
    "Method: the learning will be dividing in 2 stages:\n",
    "\n",
    "* **1st stage : GMM** We chose to look for 10 clusters based on the answers on each themes, so 40 clusters will be made at the end of this stage.\n",
    "\n",
    "* **2nd stage : Self-Organizing Map (SOM)** We will try to find only 10 clusters based on the 40 clusters made at the first stage based on a SOM also called Kohonen Map. It is very useful since it is used for dimension reduction and we can easily visualize the final map on a 2-dimensions map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Stage: GMM\n",
    "\n",
    "We use a **Gaussian Mixture Model** (GMM) because in the clustering method, for each answer of a survey, we can tell the **probability** that it is a part of each cluster. While in other methods, you only have the best cluster. Because of that, the second clustering (SOM) on the top of the first (GMM) will have more **nuanced features**, not only the $4$ clusters in wich this person belongs but the probabilities this person belongs to each of the $40$ clusters.\n",
    "\n",
    "### Aim of this stage :\n",
    "\n",
    "As a result, the aim of this stage is to create the matrix $X$ whoses raws coresponds to the the persons who ansered to the open questions of the four surveys. And whoses columns are the probability that this person belongs to each of the $40$ clusters.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "To accelerate the calculus we will use the library `multiprocessing` that allows to share the calculus an several CPUs. Moreover, we will separate the extraction of the features of each of the four thems (that are independants).\n",
    "\n",
    "**This is the code for the preprocessing :**\n",
    "\n",
    "source files : `preprocessing-democratie.py`, `preprocessing-ecologie.py`, `preprocessing-fiscalite.py` and `preprocessing-organisation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = FeaturesExtractor()\n",
    "def extract_features():\n",
    "    for k in [0,1,2,3]:\n",
    "        ids_questions = get_ids_open_reponses(dfs[k,1])\n",
    "        ids_auth = np.sort(list(set(dfs_responses[k,1]['authorId'].values)))\n",
    "        responses = (dfs_responses[k,1][:].formattedValue.values.tolist())\n",
    "        # Extract embeddings for sentences\n",
    "        features = np.zeros((len(ids_auth), 300*len(ids_questions)))\n",
    "        for i in range(len(ids_auth)) :\n",
    "            for j in range(len(ids_questions)) :\n",
    "                response_unique = dfs_responses[k,1][dfs_responses[k,1]['authorId'] == ids_auth[i]][dfs_responses[k,              1][dfs_responses[k,1]['authorId'] == ids_auth[i]]['questionId'] == ids_questions[j]].formattedValue.values.tolist()\n",
    "                if (len(response_unique) > 0) :\n",
    "                    features[i][300*j:300*(j+1)] = s.get_features(response_unique[0])\n",
    "                else:\n",
    "                    features[i][300*j:300*(j+1)] = [0.]*300\n",
    "        np.savetxt(dfs_responses[k,0,]+'_all_questions.tsv', features, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we ran this function in four different processus with $k=0$, $k=1$, $k=2$ and $k=3$.\n",
    "\n",
    "\n",
    "**This is the code of FeatureExtractor() :**\n",
    "\n",
    "source file : `kmeans_embeddings.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text, language='french')\n",
    "\n",
    "\n",
    "class FeaturesExtractor:\n",
    "    \"\"\" Handle features extractions based on word embeddings (fasttext) \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_path: str = 'data/cc.fr.300.bin'):\n",
    "        assert model_path.endswith('.bin'), 'model_path should be a .bin file'\n",
    "        assert pathlib.Path(model_path).exists(), 'model_path does not exists'\n",
    "\n",
    "        self.stop_words = set(stopwords.words('french') +\n",
    "                              list(string.punctuation) +\n",
    "                              stop_words.get_stop_words('fr'))\n",
    "\n",
    "        print(('loading model could take a while...'\n",
    "               ' and takes up to 7GO of RAM'))\n",
    "        self.model = fastText.load_model(model_path)\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "    def get_features(self, response: str):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        assert type(response) == str, 'response must be a string'\n",
    "        words = tokenize(response)\n",
    "        # this line deletes the stopwords, keeps alphanumeric only, put the case to lower\n",
    "        # and stems the words\n",
    "        words = [self.porter.stem(x.lower()) for x in words if x not in self.stop_words and x.isalpha()]\n",
    "\n",
    "        return self.model.get_sentence_vector(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_features()` deletes the **stopwords**, keeps **alphanumeric** only, put the case to **lower** and **stems** the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this preprocessing, we have the **features for the GMM**. The next step is to fit and predict the proba for the GMM.\n",
    "\n",
    "The following code does the job for the organisation theme. The code is adapted to the four themes and run simultanously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_X(auth_index):\n",
    "    global gmm\n",
    "    global ids_auth\n",
    "    global features\n",
    "    global four_surveys_taken_auth_ids\n",
    "    auth = four_surveys_taken_auth_ids[auth_index]\n",
    "    k = list(ids_auth).index(auth)\n",
    "    return gmm.predict_proba(features[k].reshape(1, -1))[0]\n",
    "\n",
    "\n",
    "n_compo = 10\n",
    "df_organisation = ut.read_data('data/ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.json')\n",
    "df_resp_org = get_open_reponses(df_organisation)\n",
    "df_ids_org = get_ids_open_reponses(df_organisation)\n",
    "four_surveys_taken_auth_ids = np.loadtxt(\"four_surveys_taken_auth_ids.csv\", delimiter=\",\", dtype=str)\n",
    "ids_auth = np.sort(list(set(df_resp_org['authorId'].values)))\n",
    "np.savetxt(\"ids_auth_sorted.csv\", ids_auth, delimiter=\",\", fmt=\"%s\")\n",
    "X = np.zeros((len(four_surveys_taken_auth_ids), n_compo))\n",
    "# read features\n",
    "features = np.loadtxt('responses organisation_all_questions.tsv', delimiter='\\t')\n",
    "# Fit GMM\n",
    "gmm = GaussianMixture(n_components=n_compo)\n",
    "gmm.fit(features)\n",
    "# pool\n",
    "local_pool = multiprocessing.Pool(10)\n",
    "X = np.array(local_pool.map(fill_X, range(len(four_surveys_taken_auth_ids))))\n",
    "local_pool.close()\n",
    "local_pool.join()\n",
    "np.savetxt(\"X_organisation.csv\", X, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we concatenate the four $X$ to create the features for the SOM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Stage: Self-Organizing Map\n",
    "\n",
    "According to Wikipedia, “A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction.”\n",
    "\n",
    "Here are the most important features of SOM:\n",
    "\n",
    "* **Dimensionality Reduction:** n dimensional data from input layer will be represented by two-dimensional output layer. It can be also said that SOM can compress data by dimensionality reduction.\n",
    "\n",
    "* **Competitive Learning:** Unlike most of other ANNs, SOM uses competitive learning which means only one neuron gets activated for a input vector. The winner neuron is called Best Matching Unit (BMU).\n",
    "\n",
    "* **Topological Preservation:** In SOM, the clusters of similar features are near each other.  In other words, the data points that are nearby in high dimensional space will also be near by in low dimensional representation, hence preserving the topography. This is made possible by neighborhood weight adjustments. When an input vector excites the BMU, the weights of BMU as well as neighboring neurons are adjusted (depending on radius from BMU).\n",
    "\n",
    "We are going to use the output dataset of the first GMM stage learning: as an input we have for each author_id, a row of 40 colums which represents the proportion of finding itself in one of the 40 clusters. \n",
    "\n",
    "**The SOM training goal is to find regular patterns in the 40 clusters in order to make the 10 final clusters.**\n",
    "\n",
    "It exists two types of SOM training algorithms: unbatched and batched ones. The two training algortithms are implemented but we will only use the batched one since it is adapted to huge volume of datas (big data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python36.zip',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6/lib-dynload',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6/site-packages',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6/site-packages/aeosa',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/Users/maximelhoustau/.ipython']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try to find the best model of SOM: we need to first determine its size\n",
    "#import som_batched_learning as sbt\n",
    "import sys\n",
    "sys.path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
