{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project: French Grand Debat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Introduction :\n",
    "\n",
    "First, we would like to choose a problematic according to the data we collected. The aim of this project could be to answer the following questions:\n",
    "\n",
    "* What are the 5 most important ideas on each theme?\n",
    "* Can we build different profiles of people with their ideas on the 4 themes?\n",
    "* Can we predict themes about which people could be interested in if those people answered questions only about 3 on 4 themes? \n",
    " \n",
    "#### Doability :\n",
    "\n",
    "We are going to import the data, then check if the ids of people who submitted ideas on different themes are the same. If not, we won't be able to answer the second question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librairies imported\n",
    "import src.utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from src.kmeans_embeddings import FeaturesExtractor\n",
    "from sklearn.cluster import KMeans\n",
    "from src.utils import (read_data, get_open_reponses, get_ids_open_reponses)\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiscalite = ut.read_data('data/LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.json')\n",
    "df_democratie = ut.read_data('data/DEMOCRATIE_ET_CITOYENNETE.json')\n",
    "df_ecologie = ut.read_data('data/LA_TRANSITION_ECOLOGIQUE.json')\n",
    "df_organisation = ut.read_data('data/ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.json')\n",
    "\n",
    "dfs = np.array([[\"fiscalite\", df_fiscalite], [\"democratie\", df_democratie], [\"ecologie\", df_ecologie], [\"organisation\", df_organisation]])\n",
    "\n",
    "# open responses\n",
    "df_resp_fis = get_open_reponses(df_fiscalite)\n",
    "df_resp_dem = get_open_reponses(df_democratie)\n",
    "df_resp_eco = get_open_reponses(df_ecologie)\n",
    "df_resp_org = get_open_reponses(df_organisation)\n",
    "\n",
    "dfs_responses = np.array([[\"responses fiscalite\", df_resp_fis], [\"responses democratie\", df_resp_dem], [\"responses ecologie\", df_resp_eco], [\"responses organisation\", df_resp_org]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "for i in np.random.randint(len(df_fiscalite), size=5):\n",
    "    auth = df_fiscalite.loc[i, 'authorId']\n",
    "    print(\"Author ID : \" + auth)\n",
    "\n",
    "    dfs = np.array([[\"fiscalite\", df_fiscalite], [\"democratie\", df_democratie], [\"ecologie\", df_ecologie], [\"organisation\", df_organisation]])\n",
    "    for df in dfs:\n",
    "        code = df[1].loc[df[1]['authorId'] == auth, 'authorZipCode']\n",
    "        if(len(code) > 0):\n",
    "            code = code.values[0]\n",
    "            print(\"* In \" + df[0] + \" survey, author has zip code : \" + str(code))\n",
    "        else:\n",
    "            print(\"* In \" + df[0] + \" survey, author has not answered...\")\n",
    "    print(\"\\n############################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the previous lines, we can see that the `authorId` is likely to be an unique id whaterver the dataframe (zip code is the same).**\n",
    "\n",
    "### How many people answered several themes ?\n",
    "\n",
    "In order to build type-profiles, we need a large number of people who answered questions on several themes. That is what we will try to find with the following code.\n",
    "\n",
    "**Be careful, this cell takes a lot of time to run !**\n",
    "\n",
    "source file : `create_author_tabs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allAuthIds is the sets of all the authorIds\n",
    "allAuthIds = []\n",
    "for i in range(4):\n",
    "    allAuthIds.extend(set(dfs_responses[i,1]['authorId'].values))\n",
    "allAuthIds = set(allAuthIds)\n",
    "\n",
    "# all_auth_id_array is the sorted array of all the authorIds\n",
    "all_auth_id_array = np.sort(np.array(list(allAuthIds)))\n",
    "\n",
    "# auth_answers_count[i,j] is 1 if all_auth_id_array[i] has answered survey dfs[j]\n",
    "auth_answers_count = np.zeros((len(allAuthIds), 4), dtype=int)\n",
    "for j in range(4):\n",
    "    for i in range(len(all_auth_id_array)):\n",
    "        auth = all_auth_id_array[i]\n",
    "        line = dfs_responses[j,1].loc[dfs_responses[j,1]['authorId'] == auth]\n",
    "        if(len(line) > 0):\n",
    "            auth_answers_count[i,j] = auth_answers_count[i,j] + 1\n",
    "\n",
    "            \n",
    "# This cell aims to save the auth_answers_count array in auth_answers_count.csv\n",
    "np.savetxt(\"auth_answers_count.csv\", auth_answers_count, fmt='%1u', delimiter=\",\")\n",
    "\n",
    "# list of the authors that answered the four themes\n",
    "four_surveys_taken_auth_ids = [all_auth_id_array[i] for i in range(len(all_auth_id_array)) if number_of_survey_taken[i] == 4]\n",
    "\n",
    "np.savetxt(\"four_surveys_taken_auth_ids.csv\", four_surveys_taken_auth_ids, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can run this cell instead to load auth_answers_count from csv :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read auth_answers_count from auth_answers_count.csv\n",
    "auth_answers_count = np.loadtxt('auth_answers_count.csv', dtype=int ,delimiter=\",\")\n",
    "\n",
    "print(\"auth_answers_count :\")\n",
    "print(auth_answers_count)\n",
    "\n",
    "# number_of_survey_taken[i] is the number of survey answered by all_auth_id_array[i]\n",
    "number_of_survey_taken = np.sum(auth_answers_count, axis=1)\n",
    "# number_of_participants_by_survey[i] is the number of participants to survey dfs[j]\n",
    "number_of_participants_by_survey = np.sum(auth_answers_count, axis=0)\n",
    "\n",
    "print(\"#######################\")\n",
    "print(\"number of participant by survey :\")\n",
    "for i in range(4):\n",
    "    print(dfs[i,0] + \" : \" + str(number_of_participants_by_survey[i]))\n",
    "\n",
    "# number_of_participant_to_several_surveys[i] is the number of participants that have\n",
    "# answerd to i surveys out of the 4 (0<i<5)\n",
    "number_of_participant_to_several_surveys = np.bincount(number_of_survey_taken)\n",
    "\n",
    "print(\"#######################\")\n",
    "print(\"number of participant to x surveys :\")\n",
    "for i in range(5):\n",
    "    print(str(number_of_participant_to_several_surveys[i]) + \" people have participed to \"\n",
    "          + str(i) + \" different surveys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume\n",
    "\n",
    "Total number of participants by survey (open questions only):\n",
    "\n",
    "* **fiscalite :** 53961\n",
    "* **democratie :** 32424\n",
    "* **ecologie :** 42656\n",
    "* **organisation :** 34529\n",
    "\n",
    "Number of participant to $x$ surveys :\n",
    "\n",
    "* 54322 people have participated to only one survey\n",
    "* 17401 people have participated to 2 surveys\n",
    "* 9946 people have participated to 3 survey\n",
    "* 11152 people have participated to 4 survey\n",
    "\n",
    "These preliminaries let us think that what we wanted to do for this project is doable, because the number of people who answered on several themes is pretty large. We are now looking for a method to build these type-profiles.\n",
    "\n",
    "\n",
    "# General ideas about this project\n",
    "\n",
    "After having processed the data, our project will be articulated in two parts: \n",
    "\n",
    "* **Learning :** We are going to build a model able to find different groups (or trends opinion) based on clustering. We will train our model on people who answered the 4 themes. We will make some tests to determine a number of different profiles (maximum of 10 clusters) that seems relevant. Then we will check the relevance of these type profiles.\n",
    "* **Check :** We will test our model applying it to the test dataset (people who have participated to 3 surveys) to check if it is able to correctly predict the themes.\n",
    "\n",
    "Based on the result of the last point we will be able to say whether or not our goal is reached, and if the dataset given is enough to achieve this.\n",
    "\n",
    "Method: the learning will be dividing in 2 stages:\n",
    "\n",
    "* **1st stage : GMM** We chose to look for 10 clusters based on the answers on each themes, so 40 clusters will be made at the end of this stage.\n",
    "\n",
    "* **2nd stage : Self-Organizing Map (SOM)** We will try to find only 10 clusters based on the 40 clusters made at the first stage based on a SOM also called Kohonen Map. It is very useful since it is used for dimension reduction and we can easily visualize the final map on a 2-dimensions map.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Stage: GMM\n",
    "\n",
    "We use a **Gaussian Mixture Model** (GMM) because in the clustering method, for each answer of a survey, we can tell the **probability** that it is a part of each cluster. While in other methods, you only have the best cluster. Because of that, the second clustering (SOM) on the top of the first (GMM) will have more **nuanced features**, not only the $4$ clusters in wich this person belongs but the probabilities this person belongs to each of the $40$ clusters.\n",
    "\n",
    "### Aim of this stage :\n",
    "\n",
    "As a result, the aim of this stage is to create the matrix $X$ whoses raws coresponds to the the persons who ansered to the open questions of the four surveys. And whoses columns are the probability that this person belongs to each of the $40$ clusters.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "To accelerate the calculus we will use the library `multiprocessing` that allows to share the calculus an several CPUs. Moreover, we will separate the extraction of the features of each of the four thems (that are independants).\n",
    "\n",
    "**This is the code for the preprocessing :**\n",
    "\n",
    "source files : `preprocessing-democratie.py`, `preprocessing-ecologie.py`, `preprocessing-fiscalite.py` and `preprocessing-organisation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = FeaturesExtractor()\n",
    "def extract_features():\n",
    "    for k in [0,1,2,3]:\n",
    "        ids_questions = get_ids_open_reponses(dfs[k,1])\n",
    "        ids_auth = np.sort(list(set(dfs_responses[k,1]['authorId'].values)))\n",
    "        responses = (dfs_responses[k,1][:].formattedValue.values.tolist())\n",
    "        # Extract embeddings for sentences\n",
    "        features = np.zeros((len(ids_auth), 300*len(ids_questions)))\n",
    "        for i in range(len(ids_auth)) :\n",
    "            for j in range(len(ids_questions)) :\n",
    "                response_unique = dfs_responses[k,1][dfs_responses[k,1]['authorId'] == ids_auth[i]][dfs_responses[k,              1][dfs_responses[k,1]['authorId'] == ids_auth[i]]['questionId'] == ids_questions[j]].formattedValue.values.tolist()\n",
    "                if (len(response_unique) > 0) :\n",
    "                    features[i][300*j:300*(j+1)] = s.get_features(response_unique[0])\n",
    "                else:\n",
    "                    features[i][300*j:300*(j+1)] = [0.]*300\n",
    "        np.savetxt(dfs_responses[k,0,]+'_all_questions.tsv', features, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we ran this function in four different processus with $k=0$, $k=1$, $k=2$ and $k=3$.\n",
    "\n",
    "\n",
    "**This is the code of FeatureExtractor() :**\n",
    "\n",
    "source file : `kmeans_embeddings.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text, language='french')\n",
    "\n",
    "\n",
    "class FeaturesExtractor:\n",
    "    \"\"\" Handle features extractions based on word embeddings (fasttext) \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_path: str = 'data/cc.fr.300.bin'):\n",
    "        assert model_path.endswith('.bin'), 'model_path should be a .bin file'\n",
    "        assert pathlib.Path(model_path).exists(), 'model_path does not exists'\n",
    "\n",
    "        self.stop_words = set(stopwords.words('french') +\n",
    "                              list(string.punctuation) +\n",
    "                              stop_words.get_stop_words('fr'))\n",
    "\n",
    "        print(('loading model could take a while...'\n",
    "               ' and takes up to 7GO of RAM'))\n",
    "        self.model = fastText.load_model(model_path)\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "    def get_features(self, response: str):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        assert type(response) == str, 'response must be a string'\n",
    "        words = tokenize(response)\n",
    "        # this line deletes the stopwords, keeps alphanumeric only, put the case to lower\n",
    "        # and stems the words\n",
    "        words = [self.porter.stem(x.lower()) for x in words if x not in self.stop_words and x.isalpha()]\n",
    "\n",
    "        return self.model.get_sentence_vector(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_features()` deletes the **stopwords**, keeps **alphanumeric** only, put the case to **lower** and **stems** the words.\n",
    "\n",
    "After this preprocessing, we have the **features for the GMM**. The next step is to fit and predict the proba for the GMM.\n",
    "\n",
    "The following code does the job for the organisation theme. The code is adapted to the four themes and run simultanously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_X(auth_index):\n",
    "    global gmm\n",
    "    global ids_auth\n",
    "    global features\n",
    "    global four_surveys_taken_auth_ids\n",
    "    auth = four_surveys_taken_auth_ids[auth_index]\n",
    "    k = list(ids_auth).index(auth)\n",
    "    return gmm.predict_proba(features[k].reshape(1, -1))[0]\n",
    "\n",
    "\n",
    "n_compo = 10\n",
    "df_organisation = ut.read_data('data/ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.json')\n",
    "df_resp_org = get_open_reponses(df_organisation)\n",
    "df_ids_org = get_ids_open_reponses(df_organisation)\n",
    "four_surveys_taken_auth_ids = np.loadtxt(\"four_surveys_taken_auth_ids.csv\", delimiter=\",\", dtype=str)\n",
    "ids_auth = np.sort(list(set(df_resp_org['authorId'].values)))\n",
    "np.savetxt(\"ids_auth_sorted.csv\", ids_auth, delimiter=\",\", fmt=\"%s\")\n",
    "X = np.zeros((len(four_surveys_taken_auth_ids), n_compo))\n",
    "# read features\n",
    "features = np.loadtxt('responses organisation_all_questions.tsv', delimiter='\\t')\n",
    "# Fit GMM\n",
    "gmm = GaussianMixture(n_components=n_compo)\n",
    "gmm.fit(features)\n",
    "# pool\n",
    "local_pool = multiprocessing.Pool(10)\n",
    "X = np.array(local_pool.map(fill_X, range(len(four_surveys_taken_auth_ids))))\n",
    "local_pool.close()\n",
    "local_pool.join()\n",
    "np.savetxt(\"X_organisation.csv\", X, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we concatenate the four $X$ to create the features for the SOM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Stage: Self-Organizing Map\n",
    "\n",
    "According to Wikipedia, “A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction.”\n",
    "\n",
    "Here are the most important features of SOM:\n",
    "\n",
    "* **Dimensionality Reduction:** n dimensional data from input layer will be represented by two-dimensional output layer. It can be also said that SOM can compress data by dimensionality reduction.\n",
    "\n",
    "* **Competitive Learning:** Unlike most of other ANNs, SOM uses competitive learning which means only one neuron gets activated for a input vector. The winner neuron is called Best Matching Unit (BMU).\n",
    "\n",
    "* **Topological Preservation:** In SOM, the clusters of similar features are near each other.  In other words, the data points that are nearby in high dimensional space will also be near by in low dimensional representation, hence preserving the topography. This is made possible by neighborhood weight adjustments. When an input vector excites the BMU, the weights of BMU as well as neighboring neurons are adjusted (depending on radius from BMU).\n",
    "\n",
    "We are going to use the output dataset of the first GMM stage learning: as an input we have for each author_id, a row of 40 colums which represents the proportion of finding itself in one of the 40 clusters. \n",
    "\n",
    "**The SOM training goal is to find regular patterns in the 40 clusters in order to make the 10 final clusters.**\n",
    "\n",
    "It exists two types of SOM training algorithms: unbatched and batched ones. The two training algortithms are implemented but we will only use the batched one since it is adapted to huge volume of datas (big data).\n",
    "\n",
    "The following cell train a high number of models randomly sized. It plots the topographic error and the quantization error of each trained model, then find the best model in term of topographic error. \n",
    "\n",
    "**It takes a lot of time to run so we just kept for the final report a single model which is the best one.**\n",
    "\n",
    "You can uncomment the lines about training and test the function by training a small number of models.\n",
    "\n",
    "source files : `som_batched_training.py`, `X.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of training model n°0\n",
      "end of training model n°1\n",
      "end of training model n°2\n",
      "end of training model n°3\n",
      "end of training model n°4\n",
      "end of training model n°5\n",
      "end of training model n°6\n",
      "end of training model n°7\n",
      "end of training model n°8\n",
      "end of training model n°9\n",
      "end of training model n°10\n",
      "end of training model n°11\n",
      "end of training model n°12\n",
      "end of training model n°13\n",
      "end of training model n°14\n",
      "end of training model n°15\n",
      "end of training model n°16\n",
      "end of training model n°17\n",
      "end of training model n°18\n",
      "end of training model n°19\n",
      "end of training model n°20\n",
      "end of training model n°21\n"
     ]
    }
   ],
   "source": [
    "#Let's try to find the best model of SOM: we need to first determine its optimal size\n",
    "import som_batched_learning as sbt\n",
    "\n",
    "X=sbt.get_X()\n",
    "\n",
    "#Training nb_models of models with size between map_min_size and map_max_size\n",
    "nb_models = 100\n",
    "sbt.training_batched_som(map_min_size=10, map_max_size=50, nb_models=nb_models, X_train=X)\n",
    "#The models are saved into the directory ./data/batched_SOM_models/\n",
    "#Every model have a name like: batch_model_xx.joblib\n",
    "\n",
    "#Find the best model: the best model is the one with the minimum topographic error\n",
    "# then if 2 models have the same topographic error, we choose the one with the best quantization error\n",
    "som = sbt.get_best_model(nb_models=nb_models)\n",
    "\n",
    "#We can also open a model with its number \n",
    "#som = sbt.open_model(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a first plot of our clusters\n",
    "sbt.plot_clusters(nb_clusters=10, sm=som)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the optimal size is 15 * 37. The topographic error is around 7%, so the dimension reduction respect pretty much the topography of the original dataset.\n",
    "\n",
    "Some clusters are smallest than others but they seem to have approximatively the same size which is good thing.\n",
    "\n",
    "Here, each cluster represents a type-profile of opinion on the 4 themes of the Grand Debat.\n",
    "\n",
    "The **prototype visualization** shows the patterns learned by the neural network which are used to determine de winning neuron of each training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the prototype virtualization\n",
    "sbt.prototype_visualization(sm=som)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Hit-map** shows how the instances are spreaded across the hexagonal lattice. The more instances lay into a cell, the more instances it is representing and hence the more we have to take it into acount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the hit map\n",
    "sbt.hit_map(sm=som)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can extract the 10 clusters into a list: each element of this list is a list which contains the author-id of a certain cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data\n",
    "sbt.get_clusters(nb_clusters=10, X_projected=X, sm=som)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't had time enough to check the relevance of our 10 typical profiles. If we had time, we would have read questions and answers on each clusters in order to make opinions profiles and test their relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python36.zip',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6/lib-dynload',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6/site-packages',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6/site-packages/aeosa',\n",
       " '/Users/maximelhoustau/anaconda3/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/Users/maximelhoustau/.ipython',\n",
       " '/usr/local/lib/python3.7/site-packages/SOMPY-1.0-py3.7.egg']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/SOMPY-1.0-py3.7.egg')\n",
    "sys.path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
